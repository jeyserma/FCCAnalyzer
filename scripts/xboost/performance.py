import argparse
import numbers
from re import I
import sys, os
import json
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import accuracy_score
import uproot
import ROOT


from sklearn.metrics import roc_curve, roc_auc_score
import pickle





def evaluate_bdt_model(df, bdt, vars_list):
    X = df[vars_list]
    print(f"--->Evaluating BDT model")
    df["BDTscore"] = bdt.predict_proba(X).tolist()
    df["BDTscore"] = df["BDTscore"].apply(lambda x: x[1])
    return df


def get_performance_metrics(bdt):
    print("------>Retrieving performance metrics")
    results = bdt.evals_result()
    epochs = len(results['validation_0']['error'])
    x_axis = range(0, epochs)
    best_iteration = bdt.best_iteration + 1
    return results, epochs, x_axis, best_iteration


def plot_metrics(df,bdt,vars_list,results, epochs, x_axis, best_iteration,mode_names,latex_mappingf,final_states):    
    if final_states == "mumu":
      label = r"$Z(\mu^+\mu^-)H$"
    elif final_states == "ee":
      label = r"$Z(e^+e^-)H$"
    else:
      exit("ERROR: Invalid final state")
    ut.create_dir(f"{loc.PLOTS}")
    plot_log_loss(results, x_axis, best_iteration,label)
    plot_classification_error(results, x_axis, best_iteration,label)
    plot_auc(results, x_axis, best_iteration,label)
    plot_roc(df,label)
    plot_bdt_score(df,label)
    plot_importance(bdt,vars_list,latex_mapping,label)
    plot_significance_scan(df,label)
    plot_efficiency(df,mode_names,label)


def plot_log_loss(results, x_axis, best_iteration,label):
    print("------>Plotting log loss")
    fig, ax = plt.subplots()
    ax.plot(x_axis, results['validation_0']['logloss'], label='Training')
    ax.plot(x_axis, results['validation_1']['logloss'], label='Validation')
    #plt.axvline(best_iteration, color="gray", label="Optimal tree number")
    ax.legend()
    plt.xlabel("Number of trees")
    plt.ylabel('Log Loss')
    ax.set_title(r'$\textbf{\textit{FCC-ee}}$ $\textbf{\textit{Simulation}}$', fontsize=16, loc='left')
    ax.set_title(label, fontsize=16, loc='right')
    plt.savefig(f"{loc.PLOTS}/log_loss.png")
    plt.savefig(f"{loc.PLOTS}/log_loss.pdf")
    plt.close()


def plot_classification_error(results, x_axis, best_iteration, label):
    print("------>Plotting classification error")
    fig, ax = plt.subplots()
    ax.plot(x_axis, results['validation_0']['error'], label='Training')
    ax.plot(x_axis, results['validation_1']['error'], label='Validation')
    #plt.axvline(best_iteration, color="gray", label="Optimal tree number")
    ax.legend()
    plt.xlabel('Number of trees')
    plt.ylabel('Classification Error')
    ax.set_title(r'$\textbf{\textit{FCC-ee}}$ $\textbf{\textit{Simulation}}$', fontsize=16, loc='left')
    ax.set_title(label, fontsize=16, loc='right')
    plt.savefig(f"{loc.PLOTS}/classification_error.png")
    plt.savefig(f"{loc.PLOTS}/classification_error.pdf")
    plt.close()


def plot_auc(results, x_axis, best_iteration, label):
    print("------>Plotting AUC")
    fig, ax = plt.subplots()
    ax.plot(x_axis, results['validation_0']['auc'], label='Training')
    ax.plot(x_axis, results['validation_1']['auc'], label='Validation')
    #plt.axvline(best_iteration, color="gray", label="Optimal tree number")
    ax.legend()
    plt.xlabel('Number of trees')
    plt.ylabel('AUC')
    ax.set_title(r'$\textbf{\textit{FCC-ee}}$ $\textbf{\textit{Simulation}}$', fontsize=16, loc='left')
    ax.set_title(label, fontsize=16, loc='right')
    plt.savefig(f"{loc.PLOTS}/auc.png")
    plt.savefig(f"{loc.PLOTS}/auc.pdf")
    plt.close()


def plot_roc(df,label):
    # plot ROC 1
    print("------>Plotting ROC")
    fig, axes = plt.subplots(1, 1, figsize=(5,5))
    #df_train = df_tot.query('valid==False')
    #df_valid =  df_tot.query("valid==True")
    eps=0.
    ax=axes
    ax.set_xlabel("$\epsilon_B$")
    ax.set_ylabel("$\epsilon_S$")
    ut.plot_roc_curve(df[df['valid']==True],  "BDTscore", ax=ax, label="Validation Sample", tpr_threshold=eps)
    ut.plot_roc_curve(df[df['valid']==False], "BDTscore", ax=ax, color="#ff7f02", tpr_threshold=eps,linestyle='--', label="Training Sample")
    plt.plot([eps, 1], [eps, 1], color='navy', lw=2, linestyle='--')
    ax.legend()
    ax.set_title(r'$\textbf{\textit{FCC-ee}}$ $\textbf{\textit{Simulation}}$', fontsize=16, loc='left')
    ax.set_title(label, fontsize=16, loc='right')
    print(f"Saving ROC plot to {loc.PLOTS}/ROC1.pdf")
    fig.savefig(f"{loc.PLOTS}/ROC1.pdf")
    fig.savefig(f"{loc.PLOTS}/ROC1.png")
    fig.savefig(f"{loc.PLOTS}/ROC1.eps")




def plot_bdt_score(df, label):
    print("------>Plotting BDT score (overtraining check)")
    
    fig, ax = plt.subplots(figsize=(8, 6))
    Bins = 20
    htype = "step"
    
    tag = ['Signal Training', 'Signal Validation', 'Background Training', 'Background Validation']
    line = ['solid', 'dashed', 'solid', 'dashed']
    color = ['red', 'red', 'blue', 'blue']
    cut = ['valid==False & isSignal==1', 'valid==True & isSignal==1', 'valid==False & isSignal!=1', 'valid==True & isSignal!=1']
    
    for (x, y, z, w) in zip(tag, line, color, cut):
        df_instance = df.query(w)
        print('--------->', x, len(df_instance), "Ratio: %.2f%%" % ((len(df_instance)/float(len(df))) * 100.0))
        ax.hist(df_instance['BDTscore'], density=True, bins=Bins, range=[0.0, 1.0], histtype=htype, label=x, linestyle=y, color=z, linewidth=1.5)
    
    plt.yscale('log')
    ax.legend(loc="upper right", fontsize="medium", frameon=False, shadow=False)
    ax.set_title(r'$\textbf{\textit{FCC-ee}}$ $\textbf{\textit{Simulation}}$', fontsize=16, loc='left')
    ax.set_title(label, fontsize=18, loc='right')

    ax.set_xlabel("BDT Score", fontsize=14, loc='right', weight='bold')  
    ax.set_ylabel("Normalized to Unity", fontsize=14, loc='top', weight='bold')  
     
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    
    ax.set_ylim(top=ax.get_ylim()[1] * 2.0)  # Increase the Y-axis space
    ax.set_xlim(left=0.0, right=1.0) 

    print("------>Plotting BDT score")
    plt.savefig(f"{loc.PLOTS}/bdt_score.png")
    plt.savefig(f"{loc.PLOTS}/bdt_score.pdf")
    plt.savefig(f"{loc.PLOTS}/bdt_score.eps")
    plt.close()

def plot_importance(bdt, vars_list, latex_mapping,label):
    print("------>Plotting feature importance")
    print("------>Plotting inportance")
    fig, ax = plt.subplots(figsize=(12, 6))

    # Get feature importances and sort them by importance
    importance = bdt.get_booster().get_score(importance_type='weight')
    sorted_importance = sorted(importance.items(), key=lambda x: x[1], reverse=False)

    # Get the sorted indices of the variables
    sorted_indices = [int(x[0][1:]) for x in sorted_importance]

    # Get the sorted variable names and their corresponding importances
    sorted_vars = [vars_list[i] for i in sorted_indices]
    sorted_values = [x[1] for x in sorted_importance]

    # Update variable names with their LaTeX versions
    sorted_vars_latex = [latex_mapping[var] for var in sorted_vars]

    # Create a DataFrame and plot the feature importances
    importance_df = pd.DataFrame({'Variable': sorted_vars_latex, 'Importance': sorted_values})
    importance_df.plot(kind='barh', x='Variable', y='Importance', legend=None, ax=ax)
    ax.set_xlabel('F-score')
    ax.set_title(r'$\textbf{\textit{FCC-ee}}$ $\textbf{\textit{simulation}}$', fontsize=16, loc='left')
    ax.set_title(label, fontsize=16, loc='right')
    print(f"------>Saved {loc.PLOTS}/Importance.pdf")
    plt.savefig(f"{loc.PLOTS}/importance.png")
    plt.savefig(f"{loc.PLOTS}/importance.pdf")
    plt.savefig(f"{loc.PLOTS}/importance.eps")
    plt.close()





def plot_significance_scan(df,label):
    print("------>Plotting Significance scan")
    #compute the significance
    df_Z = ut.Significance(df[(df['isSignal'] == 1) & (df['valid'] == True)], df[(df['isSignal'] == 0) & (df['valid'] == True)], score_column = 'BDTscore', func=ut.Z, nbins=100)
    max_index=df_Z["Z"].idxmax()
    print('max-Z: {:.2f}'.format(df_Z.loc[max_index,"Z"]), 'cut threshold: [', max_index, ']')
    fig, ax = plt.subplots(figsize=(12,8))
    plt.scatter(df_Z.index, df_Z["Z"])
    ax.scatter(x=max_index, y=df_Z.loc[max_index,"Z"], c='r', marker="*")
    plt.xlabel("BDT Score ")
    plt.ylabel("Significance")
    txt1 = Rectangle((0, 0), 1, 1, fc="w", fill=False, edgecolor='none', linewidth=0)
    txt2 = Rectangle((0, 0), 1, 1, fc="w", fill=False, edgecolor='none', linewidth=0)
    plt.legend([txt1, txt2], ('max-Z: {:.2f} cut threshold: [{:.2f}]'.format(df_Z.loc[max_index,"Z"],max_index), "$Z = S/\\sqrt{S+B}$"))
    ax.set_title(r'$\textbf{\textit{FCC-ee}}$ $\textbf{\textit{Simulation}}$', fontsize=16, loc='left')
    ax.set_title(label, fontsize=16, loc='right')
    print("------>Plotting significance scan")
    plt.savefig(f"{loc.PLOTS}/significance_scan.png")
    plt.savefig(f"{loc.PLOTS}/significance_scan.pdf")
    plt.savefig(f"{loc.PLOTS}/significance_scan.eps")
    plt.close()


def plot_efficiency(df,mode_names,label):
    
    #Plot efficiency as a function of BDT cut in each sample
    print("------>Plotting Efficiency")
    BDT_cuts = np.linspace(0,99,99)
    cut_vals = []
    eff = {}

    for cur_mode in mode_names:
      eff[cur_mode] = []

    for x in tqdm(BDT_cuts):
      cut_val = float(x)/100
      cut_vals.append(cut_val)
      for cur_mode in mode_names:
        eff[cur_mode].append(float(len(df[(df['sample'] == cur_mode) & (df['valid'] == True) & (df['BDTscore'] > cut_val)]))/float(len(df[(df['sample'] == cur_mode) & (df['valid'] == True)])))
    
    fig, ax = plt.subplots(figsize=(12,8))
    
    for cur_mode in mode_names:
      plt.plot(cut_vals, eff[cur_mode], label=cur_mode)
         
    ax.tick_params(axis='both', which='major', labelsize=25)
    plt.xlim(0,1)
    plt.xlabel("BDT score",fontsize=30)
    plt.ylabel("Efficiency",fontsize=30)
    #plt.yscale('log')
    ymin,ymax = plt.ylim()
    plt.ylim(ymin,1.3)
    plt.legend(fontsize=20, loc="best")
    plt.grid(alpha=0.4,which="both")
    ax.set_title(r'$\textbf{\textit{FCC-ee}}$ $\textbf{\textit{Simulation}}$', fontsize=16, loc='left')
    ax.set_title(label, fontsize=16, loc='right')
    plt.tight_layout()
    print("------>Plotting efficiency")
    plt.savefig(f"{loc.PLOTS}/efficiency.png")
    plt.savefig(f"{loc.PLOTS}/efficiency.pdf")
    plt.savefig(f"{loc.PLOTS}/efficiency.eps")
    plt.close()


def main():
    modes = ["mumuH","ZZ","WWmumu","Zll","egamma","gammae","gaga_mumu"]
    df = load_data()
    print_input_summary(df, mode_names)
    bdt = load_trained_model(loc)
    df = evaluate_bdt_model(df, bdt, train_vars)
    results, epochs, x_axis, best_iteration = get_performance_metrics(bdt)
    plot_metrics(df,bdt,train_vars,results, epochs, x_axis, best_iteration,mode_names,latex_mapping,final_states)


def plot_roc():

    train_predictions = bdt.predict(train_data)
    test_predictions = bdt.predict(test_data)

    # calculate the ROC curve for training and testing sets
    train_fpr, train_tpr, _ = roc_curve(train_labels, train_predictions)
    test_fpr, test_tpr, _ = roc_curve(test_labels, test_predictions)

    # calculate the ROC AUC score for training and testing sets
    train_roc_auc = roc_auc_score(train_labels, train_predictions)
    test_roc_auc = roc_auc_score(test_labels, test_predictions)

    # Plot the ROC curve
    plt.figure(figsize=(8, 6))
    plt.plot(train_fpr, train_tpr, label=f"Training ROC (AUC = {train_roc_auc:.2f})")
    plt.plot(test_fpr, test_tpr, label=f"Testing ROC (AUC = {test_roc_auc:.2f})")
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.grid()
    plt.savefig(f"{outDir}/auc.png")
    plt.savefig(f"{outDir}/auc.pdf")
    plt.close()


def plot_score():

    train_predictions = bdt.predict(train_data)
    test_predictions = bdt.predict(test_data)

    # Separate the data into signal and background samples
    train_signal_scores = train_predictions[train_labels == 1]
    train_background_scores = train_predictions[train_labels == 0]
    test_signal_scores = test_predictions[test_labels == 1]
    test_background_scores = test_predictions[test_labels == 0]

    # Plot the BDT scores for signal and background events
    plt.figure(figsize=(8, 6))
    plt.hist(train_signal_scores, bins=50, range=(0, 1), alpha=0.7, label='Training Signal', color='blue')
    plt.hist(train_background_scores, bins=50, range=(0, 1), alpha=0.7, label='Training Background', color='red')
    plt.hist(test_signal_scores, bins=50, range=(0, 1), alpha=0.5, label='Testing Signal', color='lightblue', linestyle='dashed')
    plt.hist(test_background_scores, bins=50, range=(0, 1), alpha=0.5, label='Testing Background', color='salmon', linestyle='dashed')
    plt.xlabel('BDT Score')
    plt.ylabel('Number of Events')
    plt.title('BDT Score Distribution')
    plt.legend()
    plt.grid()
    plt.savefig(f"{outDir}/score.png")
    plt.savefig(f"{outDir}/score.pdf")
    plt.close()

if __name__ == "__main__":
    outDir = "/eos/user/j/jaeyserm/www/FCCee/xgboost/"
    
    
    res = pickle.load(open("tmp/bdt_model_example.pkl", "rb"))
    
    bdt = res['model']
    train_data = res['train_data']
    test_data = res['test_data']
    train_labels = res['train_labels']
    test_labels = res['test_labels']
    
    plot_score()
    plot_roc()
    
    
    quit()
    
    df = pd.read_pickle("tmp/bdt_model_example.joblib")
    bdt = joblib.load(f"tmp/bdt_model_example.joblib")
    print_input_summary(df, mode_names)